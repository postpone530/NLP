{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/postpone530/NLP/blob/main/%EC%A0%84%EC%B2%98%EB%A6%AC_%EC%8B%A4%EC%8A%B51_%EC%98%81%EC%96%B4_220216.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHvGT2eZoOS1"
      },
      "source": [
        "## 1. 영문 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xC-phA-FoOS5",
        "outputId": "1a9dc2aa-5e8b-449e-f51d-3d41f7b20e93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQ7sFIcYoOS6",
        "outputId": "64a3db50-1fd0-425d-ee91-f8b940f8a254"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Barak', 'obama', 'likes', 'fried', 'chicken', 'very', 'much']\n",
            "['Barak', 'obama', 'likes', 'fried', 'chicken', 'very', 'much']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# word_tokenize() : 마침표와 구두점(온점, 컴마, 물음표, 세미콜론, 느낌표)으로 구분하여 토큰화\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = 'Barak obama likes fried chicken very much'\n",
        "word_tokens = word_tokenize(text)\n",
        "print(word_tokens)\n",
        "\n",
        "# WordPunctTokenizer() : 알파벳이 아닌 문자를 구분하여 토큰화\n",
        "import nltk\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "# TreebankWordTokenizer() : 정규표현식에 기반한 토큰화\n",
        "import nltk\n",
        "from nltk.tokenize import TreebankWordTokenizer as tw\n",
        "\n",
        "text = 'Barak obama likes fried chicken very much'\n",
        "word = tw().tokenize(text)\n",
        "print(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQpUD_dXoOS7"
      },
      "source": [
        "### 이 외에도 토커나이저는 매우 많다. 데이터 넣어보고 판단할것."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_Tx52anoOS7"
      },
      "source": [
        "## 2. 영문 품사 부착\n",
        "### 토큰화를 먼저 하고 진행해야한다.(안하면 글자별로 품사 부착됨)\n",
        "### nltk 홈페이지에서 태그의 목록과 뜻을 파악할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vmL_PT9oOS8",
        "outputId": "d4fc5956-ef43-4250-f4fc-f2f01c22f18a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Barak', 'NNP'), ('obama', 'MD'), ('likes', 'VB'), ('fried', 'JJ'), ('chicken', 'VB'), ('very', 'RB'), ('much', 'JJ')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "taggedToken = pos_tag(word_tokens)\n",
        "print(taggedToken)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGpH_lanoOS8"
      },
      "source": [
        "## 3. 개체명 인식(ner, named entity recognition)\n",
        "### 품사 부착 후 진행해야 한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3Cy4K1ToOS9",
        "outputId": "80aea51b-7858-4f3f-d534-4a56229e30c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Barak/NNP)\n",
            "  obama/MD\n",
            "  likes/VB\n",
            "  fried/JJ\n",
            "  chicken/VB\n",
            "  very/RB\n",
            "  much/JJ)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('words')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "\n",
        "from nltk import ne_chunk\n",
        "neToken = ne_chunk(taggedToken)\n",
        "print(neToken)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqlMwCwfoOS9"
      },
      "source": [
        "## 4. 원형 복원\n",
        "### 각 토큰의 원형을 복원하여 표준화 하는 것.\n",
        "### 단어의 변형을 원래대로 돌리는 것을 의미. 예 - beautiful -> beauty)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rI6aPOMYoOS-",
        "outputId": "d43fb93d-26a5-410d-8ba6-5fc7ff5815dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run  studi\n"
          ]
        }
      ],
      "source": [
        "# 어간추출(stemming) 방식\n",
        "# 규칙에 기반하여 토큰을 표준화한다.\n",
        "# ning제거, ful 제거 등\n",
        "# 사전에 없는 단어 등은 원형 복원이 안된다.\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "print(ps.stem(\"running\"),\"\",ps.stem(\"studies\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "syqn3HN5oOS-",
        "outputId": "708ff927-8695-4d02-8458-431647782418"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'stop'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# wordnetlammatizer 방식도 있음\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "w1 = WordNetLemmatizer\n",
        "w1.lemmatize(\"\",\"stops\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWBA91UBoOS_"
      },
      "source": [
        "## 5. 불용어 처리(stopword)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8lXlOBeoOS_",
        "outputId": "dcc07e45-fb28-4893-c519-3e43c3961e37"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('Barak', 'NNP'), 1),\n",
              " (('obama', 'MD'), 1),\n",
              " (('likes', 'VB'), 1),\n",
              " (('fried', 'JJ'), 1),\n",
              " (('chicken', 'VB'), 1),\n",
              " (('very', 'RB'), 1),\n",
              " (('much', 'JJ'), 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "stopPos = ['IN','CC','UH','TO','MD']\n",
        "\n",
        "# 최빈어 조회(를 통한 불용어 제거 대상 선정)\n",
        "from collections import Counter\n",
        "Counter(taggedToken).most_common()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suXX-fntoOS_",
        "outputId": "4c35c678-0ca7-48f5-cfd0-4ea0aa9cd268"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Barak', 'likes', 'fried', 'chicken', 'very', 'much']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "stopword = [',','be','able']\n",
        "\n",
        "word = []\n",
        "for tag in taggedToken:\n",
        "    if tag[1] not in stopPos:\n",
        "        if tag[0] not in stopword:\n",
        "            word.append(tag[0])\n",
        "\n",
        "word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0Y6UqeNooOTA"
      },
      "outputs": [],
      "source": [
        "import pandas\n",
        "pandas?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB8Ha93FoOTA"
      },
      "source": [
        "## 총 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6z96NQvoOTA",
        "outputId": "107f2424-4113-4e32-ec7a-e7f22e69becb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['pandas', 'is', 'a', 'Python', 'package', 'providing', 'fast', ',', 'flexible', ',', 'and', 'expressive', 'data', 'structures', 'designed', 'to', 'make', 'working', 'with', 'relational', 'or', 'labeled', 'data', 'both', 'easy', 'and', 'intuitive']\n",
            "[('pandas', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('Python', 'NNP'), ('package', 'NN'), ('providing', 'VBG'), ('fast', 'RB'), (',', ','), ('flexible', 'JJ'), (',', ','), ('and', 'CC'), ('expressive', 'JJ'), ('data', 'NNS'), ('structures', 'NNS'), ('designed', 'VBN'), ('to', 'TO'), ('make', 'VB'), ('working', 'VBG'), ('with', 'IN'), ('relational', 'JJ'), ('or', 'CC'), ('labeled', 'VBN'), ('data', 'NNS'), ('both', 'DT'), ('easy', 'JJ'), ('and', 'CC'), ('intuitive', 'JJ')]\n",
            "(S\n",
            "  pandas/NN\n",
            "  is/VBZ\n",
            "  a/DT\n",
            "  (GPE Python/NNP)\n",
            "  package/NN\n",
            "  providing/VBG\n",
            "  fast/RB\n",
            "  ,/,\n",
            "  flexible/JJ\n",
            "  ,/,\n",
            "  and/CC\n",
            "  expressive/JJ\n",
            "  data/NNS\n",
            "  structures/NNS\n",
            "  designed/VBN\n",
            "  to/TO\n",
            "  make/VB\n",
            "  working/VBG\n",
            "  with/IN\n",
            "  relational/JJ\n",
            "  or/CC\n",
            "  labeled/VBN\n",
            "  data/NNS\n",
            "  both/DT\n",
            "  easy/JJ\n",
            "  and/CC\n",
            "  intuitive/JJ)\n",
            "['pandas', 'is', 'Python', 'package', 'providing', 'fast', 'flexible', 'expressive', 'data', 'structures', 'designed', 'make', 'working', 'relational', 'labeled', 'data', 'easy', 'intuitive']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('words')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.tokenize import TreebankWordTokenizer as tw\n",
        "sumtoken = tw().tokenize(\"pandas is a Python package providing fast, flexible, and expressive data structures designed to make working with relational or labeled data both easy and intuitive\")\n",
        "print(sumtoken)\n",
        "\n",
        "from nltk import pos_tag\n",
        "taggedToken = pos_tag(sumtoken)\n",
        "print(taggedToken)\n",
        "\n",
        "from nltk import ne_chunk\n",
        "neToken = ne_chunk(taggedToken)\n",
        "print(neToken)\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "sumstopPos = ['IN','CC','UH','TO','MD',',','DT']\n",
        "\n",
        "sumstopWord = ['fried']\n",
        "\n",
        "word = []\n",
        "for tag in taggedToken:\n",
        "    if tag[1] not in sumstopPos:\n",
        "        if tag[0] not in sumstopWord:\n",
        "            word.append(tag[0])\n",
        "print(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yHw0C6v1oOTB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ph0uyeIVoOTB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Q2os8T9woOTB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "er7Mql9UoOTB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "M3j-pUxGoOTB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "전처리 실습1 - 영어_220216.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}